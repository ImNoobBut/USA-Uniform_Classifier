# -*- coding: utf-8 -*-
"""Model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ADcBGpyBYuIWaXbxh-LvschFPPwTM9B8
"""

!source /content/yolo/bin/activate

from google.colab import drive
drive.mount('/content/drive')

import cv2
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import DenseNet169
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import plot_model
from PIL import Image
import matplotlib.pyplot as plt

# Set random seed for reproducibility
np.random.seed(42)

# Directory paths
image_directory = r'/content/drive/Shareddrives/CPE414dataset'
uniform_directory = os.path.join(image_directory, 'Uniform')
no_uniform_directory = os.path.join(image_directory, 'NoUniform')

# Ensure directories exist
if not os.path.exists(uniform_directory) or not os.path.exists(no_uniform_directory):
    print("Error: Directories do not exist.")
    sys.exit(1)

# Hyperparameters
INPUT_SIZE = 224  # Specify the input size for DenseNet169
BATCH_SIZE = 32
EPOCHS = 50

# Data augmentation with more variety
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    brightness_range=[0.8, 1.2],
    fill_mode='nearest'
)

# Load and preprocess the dataset
def load_and_preprocess_data(directory, label):
    dataset = []
    labels = []

    image_files = os.listdir(directory)
    for image_name in image_files:
        if image_name.endswith('.jpg') or image_name.endswith('.png') or image_name.endswith('.jpeg'):
            try:
                image = cv2.imread(os.path.join(directory, image_name))
                if image is not None:
                    image = Image.fromarray(image, 'RGB')
                    image = image.resize((INPUT_SIZE, INPUT_SIZE))
                    dataset.append(np.array(image))
                    labels.append(label)
            except Exception as e:
                print(f"Error loading image {image_name}: {str(e)}")

    return np.array(dataset), np.array(labels)

data_uniform, labels_uniform = load_and_preprocess_data(uniform_directory, 1)
data_no_uniform, labels_no_uniform = load_and_preprocess_data(no_uniform_directory, 0)

# Combine data from both classes
X = np.concatenate((data_uniform, data_no_uniform), axis=0)
y = np.concatenate((labels_uniform, labels_no_uniform), axis=0)

# Split the dataset into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Normalize pixel values using DenseNet169 preprocess_input function
from tensorflow.keras.applications.densenet import preprocess_input

X_train = preprocess_input(X_train)
X_val = preprocess_input(X_val)

# Load the pre-trained DenseNet169 model
base_model = DenseNet169(weights='imagenet', include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3))

# Create a more adaptable classification head for the model
model = Sequential()
model.add(base_model)
model.add(GlobalAveragePooling2D())
model.add(Dense(256, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))  # Adding dropout for regularization
model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())
model.add(Dense(2, activation='softmax'))  # 2 output units for "uniform" and "no_uniform"

# Fine-tune all layers of the base model
for layer in base_model.layers:
    layer.trainable = True

# Use a learning rate schedule
initial_learning_rate = 0.0001
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps=10000, decay_rate=0.9, staircase=True
)

# Compile the model with a learning rate schedule
model.compile(
    loss='categorical_crossentropy',
    optimizer=Adam(learning_rate=lr_schedule),
    metrics=['accuracy']
)

# Set up callbacks
early_stopping = EarlyStopping(patience=20, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('models/uniform_DenseNet169_bestv3.h5', save_best_only=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)

# One-hot encode labels
y_train_one_hot = to_categorical(y_train, num_classes=2)

# Calculate class weights to handle imbalance
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weights_dict = dict(enumerate(class_weights))

# Train the model with calculated class weights
history = model.fit(
    datagen.flow(X_train, y_train_one_hot, batch_size=BATCH_SIZE),
    steps_per_epoch=len(X_train) / BATCH_SIZE,
    epochs=EPOCHS,
    validation_data=(X_val, to_categorical(y_val, num_classes=2)),
    callbacks=[early_stopping, model_checkpoint, reduce_lr],
    class_weight=class_weights_dict
)

# Save the final model
model.save('models/uniform_DenseNet169_finalv3.h5')

# Display the model summary
model.summary()

# Plot the model architecture to a file
plot_model(model, to_file='model_architecture_DenseNet169v3.png', show_shapes=True)

# Plot training history
plt.figure(figsize=(12, 5))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

# Test set evaluation
test_directory = r'/content/drive/Shareddrives/CPE414dataset/NoUniform'
test_label = r'/content/drive/Shareddrives/CPE414dataset/Uniform'

# Evaluate the model on the test set
X_test, y_test = load_and_preprocess_data(test_directory, test_label)
X_test = preprocess_input(X_test)
y_test_one_hot = to_categorical(y_test, num_classes=2)

# Calculate class weights for the test set
class_weights_test = compute_class_weight('balanced', np.unique(y_test), y_test)
class_weights_test_dict = dict(enumerate(class_weights_test))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot, class_weight=class_weights_test_dict)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Generate classification report
y_pred_test = np.argmax(model.predict(X_test), axis=1)
print("Classification Report:")
print(classification_report(y_test, y_pred_test))

import os

def rename_files(folder_path):
    # Get the list of files in the folder
    files = os.listdir(folder_path)

    # Specify the desired prefix for the new file names
    prefix = "Y"

    # Iterate through each file and rename it
    for index, file_name in enumerate(files):
        # Construct the new file name
        new_name = f"{prefix}_{index + 1}{os.path.splitext(file_name)[1]}"

        # Create the full paths for the old and new file names
        old_path = os.path.join(folder_path, file_name)
        new_path = os.path.join(folder_path, new_name)

        # Rename the file
        os.rename(old_path, new_path)

        print(f"Renamed: {file_name} -> {new_name}")

if __name__ == "__main__":
    # Specify the path to the folder containing the files
    folder_path = r"/content/drive/Shareddrives/CPE414dataset/Uniform"

    # Call the function to rename files in the specified folder
    rename_files(folder_path)

import cv2
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import DenseNet169
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import to_categorical
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report

# Function to load and preprocess data
def load_and_preprocess_data(directory, label):
    dataset = []
    labels = []

    image_files = os.listdir(directory)
    for image_name in image_files:
        if image_name.endswith('.jpg') or image_name.endswith('.png'):
            try:
                image = cv2.imread(os.path.join(directory, image_name))
                if image is not None:
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB
                    image = cv2.resize(image, (224, 224))
                    dataset.append(image)
                    labels.append(label)
            except Exception as e:
                print(f"Error loading image {image_name}: {str(e)}")

    return np.array(dataset), np.array(labels)

# Test set evaluation
test_directory_no_uniform = r'/content/drive/Shareddrives/CPE414dataset/NoUniform'
test_label_no_uniform = 0  # Numeric label for 'No Uniform'
X_test_no_uniform, y_test_no_uniform = load_and_preprocess_data(test_directory_no_uniform, test_label_no_uniform)

test_directory_uniform = r'/content/drive/Shareddrives/CPE414dataset/Uniform'
test_label_uniform = 1  # Numeric label for 'Uniform'
X_test_uniform, y_test_uniform = load_and_preprocess_data(test_directory_uniform, test_label_uniform)

# Concatenate the test sets and labels
X_test = np.concatenate((X_test_no_uniform, X_test_uniform), axis=0)
y_test = np.concatenate((y_test_no_uniform, y_test_uniform), axis=0)

# Check if the test dataset is empty
if len(X_test) == 0:
    print("Error: No valid images found in the test directory.")
    exit(1)

# Print debug information
print(f"Number of images in the test set: {len(X_test)}")

# Normalize pixel values using DenseNet169 preprocess_input function
X_test = tf.keras.applications.densenet.preprocess_input(X_test)
y_test_one_hot = to_categorical(y_test, num_classes=2)

# Load the trained model
model = load_model('models/uniform_DenseNet169_finalv1.h5')
# Weigh the samples based on class weights
sample_weights = np.array([class_weights_test_dict[label] for label in y_test])

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot, sample_weight=sample_weights)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# Generate classification report
y_pred_test = np.argmax(model.predict(X_test), axis=1)
print("Classification Report:")
print(classification_report(y_test, y_pred_test, target_names=["No Uniform", "Uniform"]))